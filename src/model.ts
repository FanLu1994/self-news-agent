import { complete, getModel } from '@mariozechner/pi-ai';
import type { Context } from '@mariozechner/pi-ai';

export interface ModelConfig {
  provider: string;
  model: string;
}

function parseCandidates(raw: string | undefined): ModelConfig[] {
  if (!raw) return [];
  return raw
    .split(',')
    .map(item => item.trim())
    .filter(Boolean)
    .map(item => {
      const [provider, ...rest] = item.split(':');
      return {
        provider: provider?.trim() || '',
        model: rest.join(':').trim()
      };
    })
    .filter(item => item.provider && item.model);
}

export function getConfiguredModel(): ModelConfig {
  return {
    provider: process.env.LLM_PROVIDER || 'openai',
    model: process.env.LLM_MODEL || 'deepseek-chat'
  };
}

/**
 * è·å– pi-ai æ¨¡å‹å¯¹è±¡ï¼ˆç”¨äº Agent ç­‰éœ€è¦å®é™…æ¨¡å‹å¯¹è±¡çš„åœºæ™¯ï¼‰
 */
export function getPiAiModel() {
  const config = getConfiguredModel();

  // DeepSeek éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œå› ä¸º pi-ai ä¸ç›´æ¥æ”¯æŒ
  if (config.provider === 'openai' && config.model === 'deepseek-chat') {
    // DeepSeek é€šè¿‡ completeWithFallback ç›´æ¥è°ƒç”¨ï¼Œè¿™é‡Œè¿”å› null
    // Agent æ¨¡å¼ä¼šä½¿ç”¨å¤‡ç”¨æ¨¡å‹
    const candidates = getModelCandidates().filter(
      c => !(c.provider === config.provider && c.model === config.model)
    );
    const fallback = candidates[0];
    if (fallback) {
      return getModel(fallback.provider, fallback.model);
    }
  }

  return getModel(config.provider, config.model);
}

export function getModelCandidates(): ModelConfig[] {
  const envCandidates = parseCandidates(process.env.LLM_CANDIDATES);
  if (envCandidates.length > 0) return envCandidates;

  return [
    { provider: 'openai', model: 'deepseek-chat' },
    { provider: 'zai', model: 'glm-4.7' },
    { provider: 'openai', model: 'gpt-4o' },
    { provider: 'anthropic', model: 'claude-sonnet-4-20250514' },
    { provider: 'google', model: 'gemini-2.5-flash' }
  ];
}

export async function completeWithFallback(context: Context): Promise<{
  response: Awaited<ReturnType<typeof complete>>;
  config: ModelConfig;
}> {
  const primary = getConfiguredModel();
  const candidates = [primary, ...getModelCandidates().filter(item =>
    !(item.provider === primary.provider && item.model === primary.model)
  )];

  console.log(`\nğŸ”§ LLM é…ç½®:`);
  console.log(`  ä¸»æ¨¡å‹: ${primary.provider}:${primary.model}`);
  console.log(`  å¤‡ç”¨æ¨¡å‹: ${candidates.slice(1).map(c => `${c.provider}:${c.model}`).join(', ')}`);

  let lastError: unknown;
  for (const [index, candidate] of candidates.entries()) {
    try {
      console.log(`\n  ğŸ“¡ å°è¯•æ¨¡å‹ ${index + 1}/${candidates.length}: ${candidate.provider}:${candidate.model}`);

      let response: any;

      // DeepSeek ä½¿ç”¨è‡ªå®šä¹‰ API
      if (candidate.provider === 'openai' && candidate.model === 'deepseek-chat') {
        const baseUrl = process.env.OPENAI_BASE_URL || 'https://api.deepseek.com';
        const apiKey = process.env.OPENAI_API_KEY;
        console.log(`    ä½¿ç”¨ DeepSeek API: ${baseUrl}`);

        const apiResponse = await fetch(`${baseUrl}/chat/completions`, {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${apiKey}`
          },
          body: JSON.stringify({
            model: 'deepseek-chat',
            messages: context.messages.map((m: any) => ({
              role: m.role,
              content: m.content
            })),
            stream: false
          })
        });

        if (!apiResponse.ok) {
          const errorText = await apiResponse.text();
          throw new Error(`DeepSeek API error: ${apiResponse.status} ${errorText}`);
        }

        const data = await apiResponse.json();
        response = {
          content: [{
            type: 'text',
            text: data.choices[0]?.message?.content || ''
          }],
          usage: data.usage
        };
      } else {
        // å…¶ä»–æ¨¡å‹ä½¿ç”¨ pi-ai çš„ complete
        let model = getModel(candidate.provider, candidate.model);
        model = applyDeepSeekBaseUrl(model);

        console.log(`    æ¨¡å‹ ID: ${model?.id || 'N/A'}`);
        console.log(`    æ¨¡å‹ baseUrl: ${model?.baseUrl || 'N/A'}`);

        response = await complete(model, context);
      }

      console.log(`    âœ… æˆåŠŸ!`);
      console.log(`    å“åº” blocks: ${response.content?.length || 0}`);

      // æ£€æŸ¥å“åº”å†…å®¹
      const textBlocks = response.content?.filter((b: any) => b.type === 'text') || [];
      console.log(`    æ–‡æœ¬ blocks: ${textBlocks.length}`);
      if (textBlocks.length > 0) {
        const firstText = textBlocks[0].text || '';
        console.log(`    é¦–ä¸ª block é•¿åº¦: ${firstText.length}`);
        console.log(`    é¦–ä¸ª block é¢„è§ˆ: ${firstText.slice(0, 200)}...`);
      }

      return { response, config: candidate };
    } catch (error) {
      console.log(`    âŒ å¤±è´¥: ${error instanceof Error ? error.message : String(error)}`);
      if (error instanceof Error && error.stack) {
        console.log(`    Stack: ${error.stack.split('\n').slice(0, 3).join('\n')}`);
      }
      lastError = error;
    }
  }

  console.error(`\nâŒ æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥äº†:`);
  console.error(`  æœ€åé”™è¯¯: ${lastError instanceof Error ? lastError.message : String(lastError)}`);

  throw lastError instanceof Error ? lastError : new Error('No available model candidate succeeded.');
}
